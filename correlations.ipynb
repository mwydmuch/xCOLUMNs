{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "marg_dist_A = [\n",
    "    [0.6, 0.5, 0.2], # x1\n",
    "    [0.1, 0.2, 0.5], # x2\n",
    "    #[0.2, 0.3, 0.4], # x3\n",
    "]\n",
    "\n",
    "def marg2joint(marg_dist):\n",
    "    joint_dist = []\n",
    "    for x in marg_dist:\n",
    "        joint_dist.append([])\n",
    "        for v in itertools.product([0, 1], repeat=len(x)):\n",
    "            v = np.array(v)\n",
    "            joint_p = 1\n",
    "            for l, p in zip(v, x):\n",
    "                joint_p *= p * l + (1 - p) * (1 - l)\n",
    "            joint_dist[-1].append((v, joint_p))\n",
    "\n",
    "    return joint_dist\n",
    "\n",
    "\n",
    "def joint2marg(joint_dist):\n",
    "    marg_dist = []\n",
    "    for x in joint_dist:\n",
    "        marg_p = np.zeros(3)\n",
    "        for (v, p) in x:\n",
    "            marg_p += v * p\n",
    "        marg_dist.append(marg_p)\n",
    "    return marg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_A = marg2joint(marg_dist_A)\n",
    "print(joint_A)\n",
    "print(joint2marg(joint_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dist_B = [\n",
    "    [\n",
    "        [[(array([0, 0, 0]), 0.16000000000000003), (array([0, 0, 1]), 0.04000000000000001), (array([0, 1, 0]), 0.16000000000000003), (array([0, 1, 1]), 0.04000000000000001), (array([1, 0, 0]), 0.24), (array([1, 0, 1]), 0.06), (array([1, 1, 0]), 0.24), (array([1, 1, 1]), 0.06)], [(array([0, 0, 0]), 0.36000000000000004), (array([0, 0, 1]), 0.36000000000000004), (array([0, 1, 0]), 0.09000000000000001), (array([0, 1, 1]), 0.09000000000000001), (array([1, 0, 0]), 0.04000000000000001), (array([1, 0, 1]), 0.04000000000000001), (array([1, 1, 0]), 0.010000000000000002), (array([1, 1, 1]), 0.010000000000000002)]]\n",
    "        ((0, 0, 0),)\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "from bca_prediction import *\n",
    "from random import randint\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "\n",
    "FLOAT_TYPE=np.float32\n",
    "IND_TYPE=np.int32\n",
    "\n",
    "\n",
    "def predict_g(eta_pred, G):\n",
    "    return (eta_pred.data * (G[:,0][eta_pred.indices] - G[:,1][eta_pred.indices] - G[:,2][eta_pred.indices])) + G[:,1][eta_pred.indices]\n",
    "\n",
    "\n",
    "def predict_top_k(eta_pred, G, k):\n",
    "    \"\"\"\n",
    "    Predicts the labels for a given gradient matrix G and probability estimates eta_pred\n",
    "    \"\"\"\n",
    "    ni = eta_pred.shape[0]\n",
    "    result_data = np.ones(ni * k, dtype=FLOAT_TYPE)\n",
    "    result_indices = np.zeros(ni * k, dtype=IND_TYPE)\n",
    "    result_indptr = np.zeros(ni + 1, dtype=IND_TYPE)\n",
    "    for i in trange(ni):\n",
    "        eta_i = eta_pred[i]\n",
    "        g = predict_g(eta_i, G)\n",
    "        top_k = np.argpartition(-g, k)[:k]\n",
    "        result_indices[i * k:(i + 1) * k] = sorted(eta_i.indices[top_k])\n",
    "        result_indptr[i + 1] = result_indptr[i] + k\n",
    "\n",
    "    return csr_matrix((result_data, result_indices, result_indptr), shape=(ni, G.shape[0]))\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix(pred_labels, true_labels, C_shape):\n",
    "    \"\"\"\n",
    "    Calculate normalized confusion matrix\n",
    "    \"\"\"\n",
    "    C = np.zeros(C_shape, dtype=torch.float32)\n",
    "    C[:, 0] = np.sum(pred_labels * true_labels, axis=0)\n",
    "    C[:, 1] = np.sum((1 - pred_labels) * true_labels, axis=0)\n",
    "    C[:, 2] = np.sum(pred_labels * (1 - true_labels), axis=0)\n",
    "    C = C / true_labels.shape[0]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "def calculate_loss(fn, C):\n",
    "    C = torch.tensor(C, dtype=torch.float32)\n",
    "    loss = fn(C)\n",
    "    loss = torch.mean(loss)\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def calculate_loss_with_gradient(fn, C, reg):\n",
    "    C = torch.tensor(C, requires_grad=True, dtype=torch.float32)\n",
    "    loss = fn(C)\n",
    "    loss = torch.mean(loss)\n",
    "    loss.backward()\n",
    "    return float(loss), np.array(C.grad)\n",
    "\n",
    "\n",
    "def fw_macro_recall(C, epsilon=1e-5):\n",
    "    return C[:,0] / (C[:,0] + C[:,2] + epsilon)\n",
    "\n",
    "\n",
    "def fw_macro_precision(C, epsilon=1e-5):\n",
    "    return C[:,0] / (C[:,0] + C[:,1] + epsilon)\n",
    "\n",
    "\n",
    "def fw_macro_f1(C, beta=1.0, epsilon=1e-5):\n",
    "    # precision = fw_macro_precision(C, epsilon=epsilon)\n",
    "    # recall = fw_macro_recall(C, epsilon=epsilon)\n",
    "    # return (1 + beta**2) * precision * recall / (beta**2 * precision + recall + epsilon)\n",
    "    return 2 * C[:,0] / (2 * C[:,0] + C[:,1] + C[:,2] + epsilon)\n",
    "\n",
    "\n",
    "def find_alpha(C, C_i, loss_func, g=1000):\n",
    "    print(\"  Finding alpha\")\n",
    "    max_loss = 0\n",
    "    max_alpha = 0\n",
    "\n",
    "    for i in range(g):\n",
    "        alpha = i / g\n",
    "        new_C = (1 - alpha) * C + alpha * C_i\n",
    "        loss = calculate_loss(loss_func, new_C)\n",
    "        #print(f\"    Alpha: {alpha}, loss: {loss * 100}\")\n",
    "        \n",
    "        if loss > max_loss:\n",
    "            max_loss = loss\n",
    "            max_alpha = alpha\n",
    "\n",
    "    return max_alpha\n",
    "\n",
    "\n",
    "def frank_wolfe(y_true, eta_pred, max_iters=10, init=\"top\", loss_func=None, k=5, stop_on_zero=True, reg=0, **kwargs):\n",
    "    print(\"Starting Frank-Wolfe algorithm\")\n",
    "\n",
    "    m = eta_pred.shape[1]  # number of labels\n",
    "    C_shape = (eta_pred.shape[1], 3)  # 0: TP, 1: FP, 2: FN\n",
    "    init_G = np.zeros(C_shape)\n",
    "\n",
    "    print(f\"  Calculating initial loss based on {init}\")\n",
    "    if init == \"top\":\n",
    "        init_G[:, 0] = 1\n",
    "    elif init == \"random\":\n",
    "        init_G[:, 0] = np.random.rand(m)\n",
    "    init_pred = predict_top_k(eta_pred, init_G, k)\n",
    "    #print(\"True:\", y_true[0], \"Pred:\", init_pred[0])\n",
    "    print(\"True:\", y_true.shape, \"Pred:\", init_pred.shape, \"Eta:\", eta_pred.shape)\n",
    "    C = calculate_confusion_matrix(init_pred, y_true, C_shape)\n",
    "    loss = calculate_loss(loss_func, C)\n",
    "    print(f\"  Initial loss: {loss * 100}\")\n",
    "    \n",
    "    classifiers = np.zeros((max_iters,) + C_shape)\n",
    "    classifier_weights = np.zeros(max_iters)\n",
    "\n",
    "    classifiers[0] = init_G  \n",
    "    classifier_weights[0] = 1\n",
    "\n",
    "    for i in range(1, max_iters):\n",
    "        print(f\"Starting iteration {i} ...\")\n",
    "        loss, G = calculate_loss_with_gradient(loss_func, C, reg)\n",
    "        print(f\"  Loss: {loss * 100}\")\n",
    "        # print(f\"  Gradients: {G}\")\n",
    "        # print(f\"  Grad sum {np.sum(G, axis=1)}\")\n",
    "        # print(f\"  C matrix: {C}\")\n",
    "        \n",
    "        classifiers[i] = G\n",
    "        pred = predict_top_k(eta_pred, G, k)\n",
    "        C_i = calculate_confusion_matrix(pred, y_true, C_shape)\n",
    "        loss_i = calculate_loss(loss_func, C_i)\n",
    "        print(f\"  Loss_i: {loss_i * 100}\")\n",
    "        \n",
    "        alpha = find_alpha(C, C_i, loss_func)\n",
    "        #alpha = 1\n",
    "        #alpha = 2 / (i + 2)\n",
    "        \n",
    "        classifier_weights[:i] *= (1 - alpha)\n",
    "        classifier_weights[i] = alpha\n",
    "        C = (1 - alpha) * C + alpha * C_i\n",
    "\n",
    "        print(f\"  Alpha: {alpha}\")\n",
    "        #print(f\"  C_i matrix : {C_i}\")\n",
    "        #print(f\"  new C matrix : {C}\")\n",
    "\n",
    "        # loss = calculate_loss(loss_func, C)\n",
    "        # print(f\"  Loss: {loss * 100}\")\n",
    "        # sampled_loss = sample_loss_from_classfiers(eta_pred, classifiers, classifier_weights, loss_func, y_true, C_shape, k=k, s=10)\n",
    "        # print(f\"  Sampled loss: {sampled_loss* 100}\")\n",
    "\n",
    "        if alpha == 0 and stop_on_zero:\n",
    "            print(\"  Alpha is zero, stopping\")\n",
    "            classifiers = classifiers[:i]\n",
    "            classifier_weights = classifier_weights[:i]\n",
    "            break\n",
    "\n",
    "    # Final loss calculation\n",
    "    final_loss = calculate_loss(loss_func, C)\n",
    "    print(f\"  Final loss: {final_loss * 100}\")\n",
    "\n",
    "    # sampled_loss = sample_loss_from_classfiers(eta_pred, classifiers, classifier_weights, loss_func, y_true, C_shape, k=k)\n",
    "    # print(f\"  Final sampled loss: {sampled_loss* 100}\")\n",
    "    \n",
    "    return classifiers, classifier_weights\n",
    "\n",
    "\n",
    "def predict_top_k_for_classfiers(eta_pred, classifiers, classifier_weights, k=5, seed=0):\n",
    "    if seed is not None:\n",
    "        #print(f\"  Using seed: {seed}\")\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    ni = eta_pred.shape[0]\n",
    "    result_data = np.ones(ni * k, dtype=FLOAT_TYPE)\n",
    "    result_indices = np.zeros(ni * k, dtype=IND_TYPE)\n",
    "    result_indptr = np.zeros(ni + 1, dtype=IND_TYPE)\n",
    "    for i in trange(ni):\n",
    "        c = np.random.choice(classifiers.shape[0], p=classifier_weights)\n",
    "        G = classifiers[c]\n",
    "        eta_i = eta_pred[i]\n",
    "        g = predict_g(eta_i, G)\n",
    "        top_k = np.argpartition(-g, k)[:k]\n",
    "        result_indices[i * k:(i + 1) * k] = sorted(eta_i.indices[top_k])\n",
    "        result_indptr[i + 1] = result_indptr[i] + k\n",
    "\n",
    "    return csr_matrix((result_data, result_indices, result_indptr), shape=(ni, G.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "def sample_loss_from_classfiers(eta_pred, classifiers, classifier_weights, loss_func, y_true, C_shape, k=5, s=5):\n",
    "    losses = []\n",
    "    for _ in range(s):\n",
    "        classfiers_pred = predict_top_k_for_classfiers(eta_pred, classifiers, classifier_weights, k=k, seed=randint(0, 1000000))\n",
    "        classfiers_C = calculate_confusion_matrix(classfiers_pred, y_true, C_shape)\n",
    "        losses.append(calculate_loss(loss_func, classfiers_C))\n",
    "    return np.mean(losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
